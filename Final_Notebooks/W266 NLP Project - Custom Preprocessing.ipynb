{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266 Project\n",
    "\n",
    "### Adam Sayre & Erin Werner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Cleaning Method\n",
    "\n",
    "Although the dataset provides both the original content as well as a cleaned version, we want to apply our own cleaning techniques and compare how they perform in the same models.\n",
    "\n",
    "So to start we can take a look at the cleaned and original content provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erinwerner/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/erinwerner/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd \n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import emoji\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "assert(nltk.download(\"treebank\"))\n",
    "from nltk.corpus import europarl_raw\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras libraries\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#from tensorflow.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Content</th>\n",
       "      <th>Original Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>oh fuck did i wrote fil grinningfacewithsweat ...</td>\n",
       "      <td>b'RT @Davbingodav: @mcrackins Oh fuck.... did ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>imagine if that reaction guy that called jj kf...</td>\n",
       "      <td>b\"@KSIOlajidebt imagine if that reaction guy t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                            Content  \\\n",
       "0  disappointed  oh fuck did i wrote fil grinningfacewithsweat ...   \n",
       "1  disappointed                       i feel nor am i shamed by it   \n",
       "2  disappointed  i had been feeling a little bit defeated by th...   \n",
       "3         happy  imagine if that reaction guy that called jj kf...   \n",
       "4  disappointed  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                    Original Content  \n",
       "0  b'RT @Davbingodav: @mcrackins Oh fuck.... did ...  \n",
       "1                       i feel nor am i shamed by it  \n",
       "2  i had been feeling a little bit defeated by th...  \n",
       "3  b\"@KSIOlajidebt imagine if that reaction guy t...  \n",
       "4  i wouldnt feel burdened so that i would live m...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"~/Downloads/dataset(clean).csv\") \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cleaned content does not include any of the website links or user tags. The cleaned content also includes all of the emoji names as a single token.\n",
    "\n",
    "#### Custom Preprocessing Technique #1\n",
    "\n",
    "So, for our personal cleaning technique, we are going to make several changes to the original content. First, we are going to clean the text of special characters, remove stopwords, and lower the text. Then, we are going to replace user tags and website instances with the token 'USERTAGINSTANCE' and 'WEBSITEINSTANCE' respectively. This is because there might be an influence in sentiment related to these Twitter interactions that can be useful in our model. These replacements will allow us to generalize these actions similar to how numbers would be replaced in other NLP tasks. Last, we will split up the emoji name descriptions into individual tokens. This is because each name contains phrases that might be more influential as individual tokens compared to as a single token. Therefore, this cleaning approach will have different results compared to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['E_Content'] = data['Original Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([i for i in re.sub(r'[^a-zA-Z\\s]', \" \", raw_text).lower().split() if i not in stopword_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data)):\n",
    "    tweet = data['E_Content'][i]\n",
    "    tweet = re.sub('b\\'','',tweet)\n",
    "    tweet = re.sub('b\\\"','',tweet)\n",
    "    tweet = re.sub('@[^\\s]+','USERTAGINSTANCE',tweet)\n",
    "    tweet = re.sub('https','WEBSITEINSTANCE',tweet)\n",
    "    tweet = preprocess(tweet)\n",
    "    \n",
    "    if i%2000 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    data['E_Content'][i] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na_index = data_e[pd.isna(data_e['E_Content'])].index\n",
    "\n",
    "#for n in na_index:\n",
    "#    data_e['E_Content'][n] = data_e['Content'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv(\"~/Downloads/dataset(clean)_e.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Content</th>\n",
       "      <th>Original Content</th>\n",
       "      <th>E_Content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>oh fuck did i wrote fil grinningfacewithsweat ...</td>\n",
       "      <td>b'RT @Davbingodav: @mcrackins Oh fuck.... did ...</td>\n",
       "      <td>rt usertaginstance usertaginstance oh fuck wro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>feel shamed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>feeling little bit defeated steps faith would ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "      <td>imagine if that reaction guy that called jj kf...</td>\n",
       "      <td>b\"@KSIOlajidebt imagine if that reaction guy t...</td>\n",
       "      <td>usertaginstance imagine reaction guy called jj...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>wouldnt feel burdened would live life testamen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1       Emotion  \\\n",
       "0           0             0  disappointed   \n",
       "1           1             1  disappointed   \n",
       "2           2             2  disappointed   \n",
       "3           3             3         happy   \n",
       "4           4             4  disappointed   \n",
       "\n",
       "                                             Content  \\\n",
       "0  oh fuck did i wrote fil grinningfacewithsweat ...   \n",
       "1                       i feel nor am i shamed by it   \n",
       "2  i had been feeling a little bit defeated by th...   \n",
       "3  imagine if that reaction guy that called jj kf...   \n",
       "4  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                    Original Content  \\\n",
       "0  b'RT @Davbingodav: @mcrackins Oh fuck.... did ...   \n",
       "1                       i feel nor am i shamed by it   \n",
       "2  i had been feeling a little bit defeated by th...   \n",
       "3  b\"@KSIOlajidebt imagine if that reaction guy t...   \n",
       "4  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                           E_Content  label  \n",
       "0  rt usertaginstance usertaginstance oh fuck wro...      0  \n",
       "1                                        feel shamed      0  \n",
       "2  feeling little bit defeated steps faith would ...      0  \n",
       "3  usertaginstance imagine reaction guy called jj...      1  \n",
       "4  wouldnt feel burdened would live life testamen...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_e = pd.read_csv(\"~/Downloads/dataset(clean)_e.csv\") \n",
    "data_e.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Preprocessing Technique #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some starting variables\n",
    "vocab_size = 10000\n",
    "max_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Original Content'].to_numpy()\n",
    "y = data.Emotion.to_numpy()\n",
    "\n",
    "# First split the data into train and test\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "# Next split the train data into train and dev data\n",
    "X_train_a, X_dev_a, y_train_a, y_dev_a = train_test_split(X_train_a, y_train_a, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "tk = Tokenizer(num_words = vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n', lower=True, split = \" \")\n",
    "tk.fit_on_texts(X_train_a)\n",
    "\n",
    "X_train_seq = tk.texts_to_sequences(X_train_a)\n",
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "\n",
    "X_dev_seq = tk.texts_to_sequences(X_dev_a)\n",
    "X_dev_seq_trunc = pad_sequences(X_dev_seq, maxlen=max_length)\n",
    "\n",
    "X_test_seq = tk.texts_to_sequences(X_test_a)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Encoding output variable\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train_le = le.fit_transform(y_train_a)\n",
    "y_train_emb = to_categorical(y_train_le)\n",
    "\n",
    "y_dev_le = le.transform(y_dev_a)\n",
    "y_dev_emb = to_categorical(y_dev_le)\n",
    "\n",
    "y_test_le = le.transform(y_test_a)\n",
    "y_test_emb = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these for training!\n",
    "X_train_final_a = X_train_seq_trunc\n",
    "X_dev_final_a = X_dev_seq_trunc\n",
    "X_test_final_a = X_test_seq_trunc\n",
    "\n",
    "y_train_final_a = y_train_emb\n",
    "y_dev_final_a = y_dev_emb\n",
    "y_test_final_a = y_test_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['A_Content'] = data['Original Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(raw_text):\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([i for i in re.sub(r'!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n', \" \", raw_text).lower().split() if i not in stopword_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n",
      "22000\n",
      "24000\n",
      "26000\n",
      "28000\n",
      "30000\n",
      "32000\n",
      "34000\n",
      "36000\n",
      "38000\n",
      "40000\n",
      "42000\n",
      "44000\n",
      "46000\n",
      "48000\n",
      "50000\n",
      "52000\n",
      "54000\n",
      "56000\n",
      "58000\n",
      "60000\n",
      "62000\n",
      "64000\n",
      "66000\n",
      "68000\n",
      "70000\n",
      "72000\n",
      "74000\n",
      "76000\n",
      "78000\n",
      "80000\n",
      "82000\n",
      "84000\n",
      "86000\n",
      "88000\n",
      "90000\n",
      "92000\n",
      "94000\n",
      "96000\n",
      "98000\n",
      "100000\n",
      "102000\n",
      "104000\n",
      "106000\n",
      "108000\n",
      "110000\n",
      "112000\n",
      "114000\n",
      "116000\n",
      "118000\n",
      "120000\n",
      "122000\n",
      "124000\n",
      "126000\n",
      "128000\n",
      "130000\n",
      "132000\n",
      "134000\n",
      "136000\n",
      "138000\n",
      "140000\n",
      "142000\n",
      "144000\n",
      "146000\n",
      "148000\n",
      "150000\n",
      "152000\n",
      "154000\n",
      "156000\n",
      "158000\n",
      "160000\n",
      "162000\n",
      "164000\n",
      "166000\n",
      "168000\n",
      "170000\n",
      "172000\n",
      "174000\n",
      "176000\n",
      "178000\n",
      "180000\n",
      "182000\n",
      "184000\n",
      "186000\n",
      "188000\n",
      "190000\n",
      "192000\n",
      "194000\n",
      "196000\n",
      "198000\n",
      "200000\n",
      "202000\n",
      "204000\n",
      "206000\n",
      "208000\n",
      "210000\n",
      "212000\n",
      "214000\n",
      "216000\n",
      "218000\n",
      "220000\n",
      "222000\n",
      "224000\n",
      "226000\n",
      "228000\n",
      "230000\n",
      "232000\n",
      "234000\n",
      "236000\n",
      "238000\n",
      "240000\n",
      "242000\n",
      "244000\n",
      "246000\n",
      "248000\n",
      "250000\n",
      "252000\n",
      "254000\n",
      "256000\n",
      "258000\n",
      "260000\n",
      "262000\n",
      "264000\n",
      "266000\n",
      "268000\n",
      "270000\n",
      "272000\n",
      "274000\n",
      "276000\n",
      "278000\n",
      "280000\n",
      "282000\n",
      "284000\n",
      "286000\n",
      "288000\n",
      "290000\n",
      "292000\n",
      "294000\n",
      "296000\n",
      "298000\n",
      "300000\n",
      "302000\n",
      "304000\n",
      "306000\n",
      "308000\n",
      "310000\n",
      "312000\n",
      "314000\n",
      "316000\n",
      "318000\n",
      "320000\n",
      "322000\n",
      "324000\n",
      "326000\n",
      "328000\n",
      "330000\n",
      "332000\n",
      "334000\n",
      "336000\n",
      "338000\n",
      "340000\n",
      "342000\n",
      "344000\n",
      "346000\n",
      "348000\n",
      "350000\n",
      "352000\n",
      "354000\n",
      "356000\n",
      "358000\n",
      "360000\n",
      "362000\n",
      "364000\n",
      "366000\n",
      "368000\n",
      "370000\n",
      "372000\n",
      "374000\n",
      "376000\n",
      "378000\n",
      "380000\n",
      "382000\n",
      "384000\n",
      "386000\n",
      "388000\n",
      "390000\n",
      "392000\n",
      "394000\n",
      "396000\n",
      "398000\n",
      "400000\n",
      "402000\n",
      "404000\n",
      "406000\n",
      "408000\n",
      "410000\n",
      "412000\n",
      "414000\n",
      "416000\n",
      "418000\n",
      "420000\n",
      "422000\n",
      "424000\n",
      "426000\n",
      "428000\n",
      "430000\n",
      "432000\n",
      "434000\n",
      "436000\n",
      "438000\n",
      "440000\n",
      "442000\n",
      "444000\n",
      "446000\n",
      "448000\n",
      "450000\n",
      "452000\n",
      "454000\n",
      "456000\n",
      "458000\n",
      "460000\n",
      "462000\n",
      "464000\n",
      "466000\n",
      "468000\n",
      "470000\n",
      "472000\n",
      "474000\n",
      "476000\n",
      "478000\n",
      "480000\n",
      "482000\n",
      "484000\n",
      "486000\n",
      "488000\n",
      "490000\n",
      "492000\n",
      "494000\n",
      "496000\n",
      "498000\n",
      "500000\n",
      "502000\n",
      "504000\n",
      "506000\n",
      "508000\n",
      "510000\n",
      "512000\n",
      "514000\n",
      "516000\n",
      "518000\n",
      "520000\n",
      "522000\n",
      "524000\n",
      "526000\n",
      "528000\n",
      "530000\n",
      "532000\n",
      "534000\n",
      "536000\n",
      "538000\n",
      "540000\n",
      "542000\n",
      "544000\n",
      "546000\n",
      "548000\n",
      "550000\n",
      "552000\n",
      "554000\n",
      "556000\n",
      "558000\n",
      "560000\n",
      "562000\n",
      "564000\n",
      "566000\n",
      "568000\n",
      "570000\n",
      "572000\n",
      "574000\n",
      "576000\n",
      "578000\n",
      "580000\n",
      "582000\n",
      "584000\n",
      "586000\n",
      "588000\n",
      "590000\n",
      "592000\n",
      "594000\n",
      "596000\n",
      "598000\n",
      "600000\n",
      "602000\n",
      "604000\n",
      "606000\n",
      "608000\n",
      "610000\n",
      "612000\n",
      "614000\n",
      "616000\n",
      "618000\n",
      "620000\n",
      "622000\n",
      "624000\n",
      "626000\n",
      "628000\n",
      "630000\n",
      "632000\n",
      "634000\n",
      "636000\n",
      "638000\n",
      "640000\n",
      "642000\n",
      "644000\n",
      "646000\n",
      "648000\n",
      "650000\n",
      "652000\n",
      "654000\n",
      "656000\n",
      "658000\n",
      "660000\n",
      "662000\n",
      "664000\n",
      "666000\n",
      "668000\n",
      "670000\n",
      "672000\n",
      "674000\n",
      "676000\n",
      "678000\n",
      "680000\n",
      "682000\n",
      "684000\n",
      "686000\n",
      "688000\n",
      "690000\n",
      "692000\n",
      "694000\n",
      "696000\n",
      "698000\n",
      "700000\n",
      "702000\n",
      "704000\n",
      "706000\n",
      "708000\n",
      "710000\n",
      "712000\n",
      "714000\n",
      "716000\n",
      "718000\n",
      "720000\n",
      "722000\n",
      "724000\n",
      "726000\n",
      "728000\n",
      "730000\n",
      "732000\n",
      "734000\n",
      "736000\n",
      "738000\n",
      "740000\n",
      "742000\n",
      "744000\n",
      "746000\n",
      "748000\n",
      "750000\n",
      "752000\n",
      "754000\n",
      "756000\n",
      "758000\n",
      "760000\n",
      "762000\n",
      "764000\n",
      "766000\n",
      "768000\n",
      "770000\n",
      "772000\n",
      "774000\n",
      "776000\n",
      "778000\n",
      "780000\n",
      "782000\n",
      "784000\n",
      "786000\n",
      "788000\n",
      "790000\n",
      "792000\n",
      "794000\n",
      "796000\n",
      "798000\n",
      "800000\n",
      "802000\n",
      "804000\n",
      "806000\n",
      "808000\n",
      "810000\n",
      "812000\n",
      "814000\n",
      "816000\n",
      "818000\n",
      "820000\n",
      "822000\n",
      "824000\n",
      "826000\n",
      "828000\n",
      "830000\n",
      "832000\n",
      "834000\n",
      "836000\n",
      "838000\n",
      "840000\n",
      "842000\n",
      "844000\n",
      "846000\n",
      "848000\n",
      "850000\n",
      "852000\n",
      "854000\n",
      "856000\n",
      "858000\n",
      "860000\n",
      "862000\n",
      "864000\n",
      "866000\n",
      "868000\n",
      "870000\n",
      "872000\n",
      "874000\n",
      "876000\n",
      "878000\n",
      "880000\n",
      "882000\n",
      "884000\n",
      "886000\n",
      "888000\n",
      "890000\n",
      "892000\n",
      "894000\n",
      "896000\n",
      "898000\n",
      "900000\n",
      "902000\n",
      "904000\n",
      "906000\n",
      "908000\n",
      "910000\n",
      "912000\n",
      "914000\n",
      "916000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(data)):\n",
    "    tweet = data['A_Content'][i]\n",
    "    tweet = preprocess(tweet)\n",
    "    \n",
    "    if i%2000 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    data['A_Content'][i] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na_index = data_a[pd.isna(data_a['A_Content'])].index\n",
    "\n",
    "#for n in na_index:\n",
    "#    data_a['A_Content'][n] = data_a['Content'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv(\"~/Downloads/dataset(clean)_a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a = pd.read_csv(\"~/Downloads/dataset(clean)_a.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Content</th>\n",
       "      <th>Original Content</th>\n",
       "      <th>A_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>oh fuck did i wrote fil grinningfacewithsweat ...</td>\n",
       "      <td>b'RT @Davbingodav: @mcrackins Oh fuck.... did ...</td>\n",
       "      <td>b rt davbingodav mcrackins oh fuck wrote fil g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>i feel nor am i shamed by it</td>\n",
       "      <td>feel shamed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>i had been feeling a little bit defeated by th...</td>\n",
       "      <td>feeling little bit defeated steps faith would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>imagine if that reaction guy that called jj kf...</td>\n",
       "      <td>b\"@KSIOlajidebt imagine if that reaction guy t...</td>\n",
       "      <td>b ksiolajidebt imagine reaction guy called jj ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>i wouldnt feel burdened so that i would live m...</td>\n",
       "      <td>wouldnt feel burdened would live life testamen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                            Content  \\\n",
       "0  disappointed  oh fuck did i wrote fil grinningfacewithsweat ...   \n",
       "1  disappointed                       i feel nor am i shamed by it   \n",
       "2  disappointed  i had been feeling a little bit defeated by th...   \n",
       "3         happy  imagine if that reaction guy that called jj kf...   \n",
       "4  disappointed  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                    Original Content  \\\n",
       "0  b'RT @Davbingodav: @mcrackins Oh fuck.... did ...   \n",
       "1                       i feel nor am i shamed by it   \n",
       "2  i had been feeling a little bit defeated by th...   \n",
       "3  b\"@KSIOlajidebt imagine if that reaction guy t...   \n",
       "4  i wouldnt feel burdened so that i would live m...   \n",
       "\n",
       "                                           A_Content  \n",
       "0  b rt davbingodav mcrackins oh fuck wrote fil g...  \n",
       "1                                        feel shamed  \n",
       "2  feeling little bit defeated steps faith would ...  \n",
       "3  b ksiolajidebt imagine reaction guy called jj ...  \n",
       "4  wouldnt feel burdened would live life testamen...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_a.head()[['Emotion','Content','Original Content','A_Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
